{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-huggingface langchain-core langchain datasets yfinance pandas faiss-cpu sentence-transformers langgraph pydantic"
      ],
      "metadata": {
        "id": "F6I6Dlk2RekU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "500c8e82"
      },
      "source": [
        "!pip install -q langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1f8d66e"
      },
      "source": [
        "try:\n",
        "    import langchain_community\n",
        "    print(f\"langchain-community is installed. Version: {langchain_community.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"langchain-community is NOT installed.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load a small slice of the dataset (streaming mode)\n",
        "dataset = load_dataset('Zihan1004/FNSPID', split='train', streaming=True)\n",
        "sample_data = []\n",
        "for i, entry in enumerate(dataset):\n",
        "    if i >= 5000: break\n",
        "    sample_data.append(entry)\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# 2. Prepare documents for the Vector Database\n",
        "docs = []\n",
        "for _, row in df.iterrows():\n",
        "    # Use the Title as the content and Symbol/Date as filters\n",
        "    doc = Document(\n",
        "        page_content=row['Article_title'],\n",
        "        metadata={\"ticker\": row['Stock_symbol'], \"date\": str(row['Date'])}\n",
        "    )\n",
        "    docs.append(doc)\n",
        "\n",
        "# 3. Create Embeddings and Vector Store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "print(\"Vector Store Ready!\")"
      ],
      "metadata": {
        "id": "1xTA49cjSHHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Fetch the secret\n",
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Set it as an environment variable so Hugging Face libraries can find it\n",
        "os.environ[\"HF_TOKEN\"] = token"
      ],
      "metadata": {
        "id": "N1ac02Pm-BRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3f0bacf"
      },
      "source": [
        "\n",
        "if \"HF_TOKEN\" in os.environ and os.environ[\"HF_TOKEN\"]:\n",
        "    print(\"Hugging Face API token is loaded successfully.\")\n",
        "else:\n",
        "    print(\"Hugging Face API token is NOT loaded. Please check your Colab secrets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa91a3e2"
      },
      "source": [
        "## Update Chatbot Query for New Feature\n",
        "\n",
        "Integrate the 'find_significant_moves_and_news' functionality into the 'chatbot_query' function. The chatbot will be updated to parse user requests for historical price moves and trigger this new analysis, returning a summarized correlation of price movements and news.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e29c6990"
      },
      "source": [
        "if 'extract_ticker_robust' in globals() and callable(globals()['extract_ticker_robust']):\n",
        "    print(\"'extract_ticker_robust' is already defined.\")\n",
        "else:\n",
        "    print(\"'extract_ticker_robust' is NOT defined. I will define it now.\")\n",
        "    # Definition of extract_ticker_robust (from cell c036e3cc)\n",
        "    def extract_ticker_robust(query, df):\n",
        "        \"\"\"Extracts stock ticker from the query. Placeholder implementation.\"\"\"\n",
        "        # A very basic regex to find what looks like a stock ticker (1-5 uppercase letters)\n",
        "        ticker_match = re.search(r'\\\\b[A-Z]{1,5}\\\\b', query)\n",
        "        if ticker_match:\n",
        "            # In a real scenario, you might want to validate this ticker against a known list\n",
        "            # For now, let's just return the first potential ticker found\n",
        "            return ticker_match.group(0)\n",
        "        return None\n",
        "    print(\"'extract_ticker_robust' has been defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9c3c59"
      },
      "source": [
        "if 'extract_ticker_robust' in globals() and callable(globals()['extract_ticker_robust']):\n",
        "    print(\"'extract_ticker_robust' is defined.\")\n",
        "else:\n",
        "    print(\"'extract_ticker_robust' is NOT defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05afa412"
      },
      "source": [
        "## Implement Supervisor Agent with Dynamic Routing\n",
        "\n",
        "Develop a Supervisor agent using LangGraph or similar state management to dynamically analyze the user's intent and decide whether to invoke the 'Plotting Agent' (e.g., for queries like 'show me a chart of...') or the 'Research Agent' (for factual or analytical queries). This will involve defining states, edges, and conditional logic within the graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84a7e94e"
      },
      "source": [
        "!pip install -q --upgrade langchain-core langgraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7b7d437"
      },
      "source": [
        "## Integrate Supervisor into Chatbot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47fe5731"
      },
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import re\n",
        "import calendar # Ensure calendar is imported for date calculations\n",
        "from langgraph.graph import StateGraph, END\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Re-initializing LLM using a local HuggingFace pipeline for clarity, but it was already initialized in a previous cell.\n",
        "model_id = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512, # Increased max_new_tokens for potentially longer summaries\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# --- Helper functions (kept as is for clarity) ---\n",
        "def extract_date_range(query):\n",
        "    \"\"\"Extracts start and end dates from a query string. Supports YYYY-MM-DD format.\n",
        "    If a YYYY-MM is found, it converts it to YYYY-MM-01 and YYYY-MM-lastday.\n",
        "    \"\"\"\n",
        "    # Try to find YYYY-MM-DD to YYYY-MM-DD\n",
        "    dates_full = re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', query)\n",
        "    if len(dates_full) == 2:\n",
        "        return dates_full[0], dates_full[1]\n",
        "    elif len(dates_full) == 1:\n",
        "        # If only one date, assume it's the start date and end date is today\n",
        "        return dates_full[0], pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "\n",
        "    # Try to find YYYY-MM (e.g., '2020-05')\n",
        "    month_year_match = re.search(r'\\b\\d{4}-\\d{2}\\b', query)\n",
        "    if month_year_match:\n",
        "        year_month_str = month_year_match.group(0)\n",
        "        year, month = map(int, year_month_str.split('-'))\n",
        "        start_date = f\"{year_month_str}-01\"\n",
        "        last_day = calendar.monthrange(year, month)[1]\n",
        "        end_date = f\"{year_month_str}-{last_day:02d}\"\n",
        "        return start_date, end_date\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def extract_threshold(query):\n",
        "    \"\"\"Extracts a percentage threshold from the query, e.g., '5%' or '5 percent'.\"\"\"\n",
        "    match = re.search(r'(\\d+)(?:%| percent)', query, re.IGNORECASE)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    return 3.0 # Default threshold\n",
        "\n",
        "def find_significant_moves_and_news(ticker, threshold, start_date, end_date, vectorstore=vectorstore):\n",
        "\n",
        "    price_data = get_price_context(ticker, start_date, end_date)\n",
        "\n",
        "    if price_data.empty:\n",
        "        return []\n",
        "\n",
        "    significant_moves = price_data[abs(price_data['Daily_Change_Percent']) >= threshold].copy()\n",
        "\n",
        "    results = []\n",
        "    for index, row in significant_moves.iterrows():\n",
        "        move_date = row['Date'].item()\n",
        "        daily_change = row['Daily_Change_Percent'].item()\n",
        "\n",
        "\n",
        "        all_news_articles = vectorstore.similarity_search(f\"News about {ticker} on {move_date}\", k=10) # Increased k\n",
        "\n",
        "        # Manually filter news articles by date metadata\n",
        "        filtered_news = []\n",
        "        for doc in all_news_articles:\n",
        "            # The date in metadata might be YYYY-MM-DD HH:MM:SS UTC, so check if it starts with move_date\n",
        "            if doc.metadata.get('date', '').startswith(move_date):\n",
        "                filtered_news.append(doc.page_content)\n",
        "\n",
        "        # Use top 3 filtered news, or all if fewer than 3\n",
        "        news_context = filtered_news[:3]\n",
        "\n",
        "        results.append({\n",
        "            'date': move_date,\n",
        "            'daily_change_percent': daily_change,\n",
        "            'news': news_context\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# --- Re-define plotting_agent and research_agent as per previous instructions ---\n",
        "def plotting_agent(user_query: str) -> str:\n",
        "    \"\"\"Placeholder function for the Plotting Agent.\"\"\"\n",
        "    return f\"Plotting Agent called for: {user_query}\"\n",
        "\n",
        "def research_agent(user_query: str) -> str:\n",
        "    \"\"\"Handles general and significant moves queries, generating a summary with LLM.\"\"\"\n",
        "\n",
        "    ticker = extract_ticker_robust(user_query, df)\n",
        "\n",
        "    if not ticker:\n",
        "        return \"Please specify a stock ticker (e.g., 'A' or 'AAPL').\"\n",
        "\n",
        "    # Check for queries related to significant moves\n",
        "    if any(keyword in user_query.lower() for keyword in ['significant moves', 'large changes', 'historical analysis', 'percentage change']):\n",
        "        start_date, end_date = extract_date_range(user_query)\n",
        "        if not start_date or not end_date:\n",
        "            return \"Please specify a valid date range (e.g., 'YYYY-MM-DD to YYYY-MM-DD' or 'YYYY-MM') for historical analysis.\"\n",
        "\n",
        "        threshold = extract_threshold(user_query)\n",
        "\n",
        "        significant_moves = find_significant_moves_and_news(ticker, threshold, start_date, end_date, vectorstore=vectorstore)\n",
        "\n",
        "        if not significant_moves:\n",
        "            return f\"No significant moves (> {threshold}%) found for {ticker} between {start_date} and {end_date}.\"\n",
        "\n",
        "        # Limit the number of significant moves shown to the LLM to prevent overly long context\n",
        "        significant_moves_limited = significant_moves[:5]\n",
        "\n",
        "        # Format significant moves for LLM (re-include concise news snippets for correlation)\n",
        "        moves_context = []\n",
        "        for move in significant_moves_limited:\n",
        "            news_snippets = \"; \".join([s[:50] + '...' if len(s) > 50 else s for s in move['news']]) # Truncate news even further\n",
        "            moves_context.append(\n",
        "                f\"On {move['date']}, {ticker} changed by {move['daily_change_percent']:.2f}%. News: {news_snippets}\"\n",
        "            )\n",
        "        moves_context_str = \"\\n\".join(moves_context)\n",
        "\n",
        "        # Refined prompt for significant moves - now using Question/Explanation format\n",
        "        prompt = f\"\"\"\n",
        "Context:\n",
        "Days with price movements greater than {threshold}% for {ticker} from {start_date} to {end_date}:\n",
        "{moves_context_str}\n",
        "\n",
        "Question: Summarize the key significant price movements and their correlation with the provided news articles. Highlight any notable patterns or events observed during this period.\n",
        "Explanation:\n",
        "\"\"\"\n",
        "        return llm.invoke(prompt)\n",
        "\n",
        "    # Existing logic for general queries\n",
        "    date_robust_str = extract_date_robust(user_query) # This returns YYYY-MM or YYYY-MM-DD or YYYY\n",
        "\n",
        "    price_info_str = \"\"\n",
        "    if date_robust_str:\n",
        "        # Convert YYYY-MM or YYYY to a range for get_price_context if necessary\n",
        "        if re.match(r'\\d{4}-\\d{2}$', date_robust_str): # YYYY-MM\n",
        "            year, month = map(int, date_robust_str.split('-'))\n",
        "            start_d = f\"{date_robust_str}-01\"\n",
        "            last_day = calendar.monthrange(year, month)[1]\n",
        "            end_d = f\"{date_robust_str}-{last_day:02d}\"\n",
        "        elif re.match(r'\\d{4}$', date_robust_str): # YYYY\n",
        "            start_d = f\"{date_robust_str}-01-01\"\n",
        "            end_d = f\"{date_robust_str}-12-31\"\n",
        "        else: # Assume YYYY-MM-DD\n",
        "            start_d = date_robust_str\n",
        "            end_d = date_robust_str # For a single day, end_date is the same\n",
        "\n",
        "        price_data_df = get_price_context(ticker, start_date=start_d, end_date=end_d)\n",
        "\n",
        "        if not price_data_df.empty:\n",
        "            # Summarize price data for the date/range\n",
        "            if start_d == end_d: # Single day query\n",
        "                day_data = price_data_df.iloc[0]\n",
        "                price_info_str = f\"{ticker} on {day_data['Date'].item()}: Closed at ${day_data['Close'].item():.2f}, Daily change: {day_data['Daily_Change_Percent'].item():.2f}%.\"\n",
        "            else: # Date range query\n",
        "                first_day = price_data_df.iloc[0]\n",
        "                last_day = price_data_df.iloc[-1]\n",
        "                price_info_str = (\n",
        "                    f\"Historical data for {ticker} from {first_day['Date'].item()} to {last_day['Date'].item()}:\\n\"\n",
        "                    f\"Start Close: ${first_day['Close'].item():.2f}, End Close: ${last_day['Close'].item():.2f}.\\n\"\n",
        "                    f\"Average daily change: {price_data_df['Daily_Change_Percent'].mean():.2f}%.\"\n",
        "                )\n",
        "        else:\n",
        "            price_info_str = f\"Could not find price data for {ticker} around {date_robust_str}.\"\n",
        "\n",
        "        search_query = user_query # Keep original query for news search context\n",
        "\n",
        "    else: # No specific date mentioned in general query\n",
        "        price_data_df = get_price_context(ticker) # Fetch recent 5 days\n",
        "        if not price_data_df.empty:\n",
        "            latest_data = price_data_df.iloc[-1]\n",
        "            price_info_str = f\"{ticker} recently: Closed at ${latest_data['Close'].item():.2f}, Daily change: {latest_data['Daily_Change_Percent'].item():.2f}%.\"\n",
        "        else:\n",
        "            price_info_str = f\"Could not find recent price data for {ticker}.\"\n",
        "        search_query = f\"Recent news about {ticker}\"\n",
        "\n",
        "\n",
        "    news_results = vectorstore.similarity_search(search_query, k=3)\n",
        "    news_context = \"\\n\".join([res.page_content for res in news_results])\n",
        "\n",
        "    # Refined prompt for general queries - now using Question/Explanation format\n",
        "    prompt = f\"\"\"\n",
        "Context:\n",
        "Price Information: {price_info_str}\n",
        "News Articles: {news_context}\n",
        "\n",
        "Question: {user_query}\n",
        "Explanation:\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "# --- Supervisor Agent Definition (copied from previous successful execution) ---\n",
        "class GraphState(BaseModel):\n",
        "    \"\"\"Represents the state of our graph.\"\"\"\n",
        "    query: str = Field(..., description=\"The user's input query\")\n",
        "    agent_outcome: str = Field(None, description=\"The outcome/response from the called agent\")\n",
        "\n",
        "def route_agent(state: GraphState) -> str:\n",
        "    \"\"\"Determines which agent to call based on the user's query.\"\"\"\n",
        "    query = state.query.lower()\n",
        "    if any(keyword in query for keyword in ['chart', 'plot', 'graph', 'visualize']):\n",
        "        return \"call_plotting_agent\"\n",
        "    # All other queries, including significant moves, go to the research agent\n",
        "    else:\n",
        "        return \"call_research_agent\"\n",
        "\n",
        "def call_plotting_agent_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"Calls the plotting agent and updates the state with its outcome.\"\"\"\n",
        "    print(f\"Calling Plotting Agent with query: {state.query}\")\n",
        "    outcome = plotting_agent(state.query)\n",
        "    return GraphState(query=state.query, agent_outcome=outcome)\n",
        "\n",
        "def call_research_agent_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"Calls the research agent and updates the state with its outcome.\"\"\"\n",
        "    print(f\"Calling Research Agent with query: {state.query}\")\n",
        "    outcome = research_agent(state.query) # Assuming research_agent now handles both general and significant moves queries\n",
        "    return GraphState(query=state.query, agent_outcome=outcome)\n",
        "\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"call_plotting_agent\", RunnableLambda(call_plotting_agent_node))\n",
        "graph.add_node(\"call_research_agent\", RunnableLambda(call_research_agent_node))\n",
        "graph.set_conditional_entry_point(\n",
        "    route_agent,\n",
        "    {\n",
        "        \"call_plotting_agent\": \"call_plotting_agent\",\n",
        "        \"call_research_agent\": \"call_research_agent\",\n",
        "    },\n",
        ")\n",
        "graph.add_edge(\"call_plotting_agent\", END)\n",
        "graph.add_edge(\"call_research_agent\", END)\n",
        "app = graph.compile()\n",
        "\n",
        "# --- Modified chatbot_query function ---\n",
        "def chatbot_query(user_query):\n",
        "    \"\"\"Delegates query processing to the Supervisor agent.\"\"\"\n",
        "    print(f\"Chatbot received query: {user_query}\")\n",
        "    # Instantiate GraphState with the user query\n",
        "    initial_state = GraphState(query=user_query)\n",
        "\n",
        "\n",
        "    result = app.invoke(initial_state)\n",
        "\n",
        "    agent_response = result['agent_outcome']\n",
        "\n",
        "    return agent_response\n",
        "\n",
        "print(\"Chatbot query function updated to use Supervisor Agent.\")\n",
        "\n",
        "# Example Usage\n",
        "print(\"\\n--- Example 1: General Query with Month-Year (routed to Research Agent) ---\")\n",
        "print(chatbot_query(\"What happened to stock A in May 2020?\"))\n",
        "\n",
        "print(\"\\n--- Example 2: Significant Moves Query (routed to Research Agent) ---\")\n",
        "print(chatbot_query(\"Show significant moves for A between 2020-01-01 and 2020-06-30 with a 5% threshold.\"))\n",
        "\n",
        "print(\"\\n--- Example 3: Plotting Query (routed to Plotting Agent) ---\")\n",
        "print(chatbot_query(\"Show me a chart of stock A's performance.\"))\n",
        "\n",
        "print(\"\\n--- Example 4: General Query without specific date (routed to Research Agent) ---\")\n",
        "print(chatbot_query(\"Tell me about stock A.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a55038"
      },
      "source": [
        "## Consolidate Setup and Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c036e3cc"
      },
      "source": [
        "!pip install -q langchain-huggingface langchain-community langchain-core\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# 4. HF_TOKEN setup\n",
        "# Fetch the secret\n",
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Set it as an environment variable so Hugging Face libraries can find it\n",
        "os.environ[\"HF_TOKEN\"] = token\n",
        "\n",
        "if \"HF_TOKEN\" in os.environ and os.environ[\"HF_TOKEN\"]:\n",
        "    print(\"Hugging Face API token is loaded successfully.\")\n",
        "else:\n",
        "    print(\"Hugging Face API token is NOT loaded. Please check your Colab secrets.\")\n",
        "\n",
        "# 5. Dataset loading and vector store creation\n",
        "# 1. Load a small slice of the dataset (streaming mode)\n",
        "dataset = load_dataset('Zihan1004/FNSPID', split='train', streaming=True)\n",
        "sample_data = []\n",
        "for i, entry in enumerate(dataset):\n",
        "    if i >= 5000: break\n",
        "    sample_data.append(entry)\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# 2. Prepare documents for the Vector Database\n",
        "docs = []\n",
        "for _, row in df.iterrows():\n",
        "    # Use the Title as the content and Symbol/Date as filters\n",
        "    doc = Document(\n",
        "        page_content=row['Article_title'],\n",
        "        metadata={\"ticker\": row['Stock_symbol'], \"date\": str(row['Date'])}\n",
        "    )\n",
        "    docs.append(doc)\n",
        "\n",
        "# 3. Create Embeddings and Vector Store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "print(\"Vector Store Ready!\")\n",
        "\n",
        "# 6. LLM initialization\n",
        "model_id = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "# --- Missing Helper functions definitions ---\n",
        "def extract_ticker_robust(query, df):\n",
        "    \"\"\"Extracts stock ticker from the query. Placeholder implementation.\"\"\"\n",
        "    # A very basic regex to find what looks like a stock ticker (1-5 uppercase letters)\n",
        "    ticker_match = re.search(r'\\b[A-Z]{1,5}\\b', query)\n",
        "    if ticker_match:\n",
        "        # In a real scenario, you might want to validate this ticker against a known list\n",
        "        # For now, let's just return the first potential ticker found\n",
        "        return ticker_match.group(0)\n",
        "    return None\n",
        "\n",
        "def get_price_context(ticker, start_date=None, end_date=None):\n",
        "    \"\"\"Fetches historical stock data using yfinance. Placeholder for full functionality.\"\"\"\n",
        "    if not ticker:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    today = datetime.now().strftime('%Y-%m-%d')\n",
        "    if not end_date: end_date = today\n",
        "    if not start_date:\n",
        "        # Default to last 5 days if no start date is provided\n",
        "        start_date = (pd.to_datetime(end_date) - pd.Timedelta(days=5)).strftime('%Y-%m-%d')\n",
        "\n",
        "    try:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
        "        if stock_data.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        stock_data['Date'] = stock_data.index.strftime('%Y-%m-%d')\n",
        "        stock_data['Daily_Change_Percent'] = stock_data['Close'].pct_change() * 100\n",
        "        # Forward fill any NaN from pct_change if it's the first day in the range\n",
        "        stock_data['Daily_Change_Percent'] = stock_data['Daily_Change_Percent'].fillna(0) # Changed this line\n",
        "        return stock_data[['Date', 'Close', 'Daily_Change_Percent']].reset_index(drop=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def extract_date_robust(query):\n",
        "    \"\"\"Extracts a date string from the query in YYYY-MM-DD, YYYY-MM, or YYYY format.\"\"\"\n",
        "    # YYYY-MM-DD\n",
        "    date_full = re.search(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', query)\n",
        "    if date_full: return date_full.group(0)\n",
        "\n",
        "    # YYYY-MM\n",
        "    date_month = re.search(r'\\b\\d{4}-\\d{2}\\b', query)\n",
        "    if date_month: return date_month.group(0)\n",
        "\n",
        "    # YYYY\n",
        "    date_year = re.search(r'\\b\\d{4}\\b', query)\n",
        "    if date_year: return date_year.group(0)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# 7. Helper functions (`extract_date_range`, `extract_threshold`, `find_significant_moves_and_news`)\n",
        "def extract_date_range(query):\n",
        "    \"\"\"Extracts start and end dates from a query string. Supports YYYY-MM-DD format.\n",
        "    If a YYYY-MM is found, it converts it to YYYY-MM-01 and YYYY-MM-lastday.\n",
        "    \"\"\"\n",
        "    # Try to find YYYY-MM-DD to YYYY-MM-DD\n",
        "    dates_full = re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', query)\n",
        "    if len(dates_full) == 2:\n",
        "        return dates_full[0], dates_full[1]\n",
        "    elif len(dates_full) == 1:\n",
        "        # If only one date, assume it's the start date and end date is today\n",
        "        return dates_full[0], pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "\n",
        "    # Try to find YYYY-MM (e.g., '2020-05')\n",
        "    month_year_match = re.search(r'\\b\\d{4}-\\d{2}\\b', query)\n",
        "    if month_year_match:\n",
        "        year_month_str = month_year_match.group(0)\n",
        "        year, month = map(int, year_month_str.split('-')) # Corrected typo here\n",
        "        start_date = f\"{year_month_str}-01\"\n",
        "        last_day = calendar.monthrange(year, month)[1]\n",
        "        end_date = f\"{year_month_str}-{last_day:02d}\"\n",
        "        return start_date, end_date\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def extract_threshold(query):\n",
        "    \"\"\"Extracts a percentage threshold from the query, e.g., '5%' or '5 percent'.\"\"\"\n",
        "    match = re.search(r'(\\d+)(?:%| percent)', query, re.IGNORECASE)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    return 3.0 # Default threshold\n",
        "\n",
        "def find_significant_moves_and_news(ticker, threshold, start_date, end_date, vectorstore=vectorstore):\n",
        "    \"\"\"Identifies dates with significant price movements and retrieves relevant news articles.\n",
        "\n",
        "    Args:\n",
        "        ticker (str): The stock ticker symbol.\n",
        "        threshold (float): The percentage threshold for significant price movement (e.g., 2 for 2%).\n",
        "        start_date (str): The start date for historical data in 'YYYY-MM-DD' format.\n",
        "        end_date (str): The end date for historical data in 'YYYY-MM-DD' format.\n",
        "        vectorstore: The FAISS vector store containing news article embeddings.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing date, daily change, and associated news.\n",
        "    \"\"\"\n",
        "    price_data = get_price_context(ticker, start_date, end_date)\n",
        "\n",
        "    if price_data.empty:\n",
        "        return []\n",
        "\n",
        "    significant_moves = price_data[abs(price_data['Daily_Change_Percent']) >= threshold].copy()\n",
        "\n",
        "    results = []\n",
        "    for index, row in significant_moves.iterrows():\n",
        "        move_date = row['Date'].item() # Fixed: Ensure move_date is a scalar string\n",
        "        daily_change = row['Daily_Change_Percent']\n",
        "\n",
        "        # Retrieve relevant news for the specific date without a regex filter\n",
        "        # Retrieve more documents and then filter them by date\n",
        "        all_news_articles = vectorstore.similarity_search(f\"News about {ticker} on {move_date}\", k=10) # Increased k\n",
        "\n",
        "        # Manually filter news articles by date metadata\n",
        "        filtered_news = []\n",
        "        for doc in all_news_articles:\n",
        "            # The date in metadata might be YYYY-MM-DD HH:MM:SS UTC, so check if it starts with move_date\n",
        "            if doc.metadata.get('date', '').startswith(move_date):\n",
        "                filtered_news.append(doc.page_content)\n",
        "\n",
        "        # Use top 3 filtered news, or all if fewer than 3\n",
        "        news_context = filtered_news[:3]\n",
        "\n",
        "        results.append({\n",
        "            'date': move_date,\n",
        "            'daily_change_percent': daily_change,\n",
        "            'news': news_context\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "# 9. Placeholder agent functions\n",
        "def plotting_agent(user_query: str) -> str:\n",
        "    \"\"\"Generates a stock price chart for the specified ticker and date range, returning it as a base64 encoded image.\"\"\"\n",
        "    ticker = extract_ticker_robust(user_query, df) # Assuming df is accessible for ticker validation if needed\n",
        "    if not ticker:\n",
        "        return \"Please specify a stock ticker (e.g., 'A' or 'AAPL') to plot.\"\n",
        "\n",
        "    start_date, end_date = extract_date_range(user_query)\n",
        "    if not start_date or not end_date:\n",
        "        # Default to a recent period if no date range is specified for plotting\n",
        "        end_date = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "        start_date = (pd.to_datetime('today') - pd.Timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "\n",
        "    price_data = get_price_context(ticker, start_date, end_date)\n",
        "\n",
        "    if price_data.empty:\n",
        "        return f\"Could not retrieve historical price data for {ticker} between {start_date} and {end_date}.\"\n",
        "\n",
        "    # Ensure 'Date' column is datetime for plotting\n",
        "    price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(price_data['Date'], price_data['Close'], label=f'{ticker} Close Price')\n",
        "    plt.title(f'{ticker} Stock Price from {start_date} to {end_date}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Close Price ($)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot to a BytesIO object\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close() # Close the plot to free up memory\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Encode to base64 string\n",
        "    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    return f\"<img src='data:image/png;base64,{image_base64}'>\"\n",
        "\n",
        "def research_agent(user_query: str) -> str:\n",
        "    \"\"\"Handles general and significant moves queries, generating a summary with LLM.\"\"\"\n",
        "\n",
        "    ticker = extract_ticker_robust(user_query, df)\n",
        "\n",
        "    if not ticker:\n",
        "        return \"Please specify a stock ticker (e.g., 'A' or 'AAPL').\"\n",
        "\n",
        "    # Check for queries related to significant moves\n",
        "    if any(keyword in user_query.lower() for keyword in ['significant moves', 'large changes', 'historical analysis', 'percentage change']):\n",
        "        start_date, end_date = extract_date_range(user_query)\n",
        "        if not start_date or not end_date:\n",
        "            return \"Please specify a valid date range (e.g., 'YYYY-MM-DD to YYYY-MM-DD' or 'YYYY-MM') for historical analysis.\"\n",
        "\n",
        "        threshold = extract_threshold(user_query)\n",
        "\n",
        "        significant_moves = find_significant_moves_and_news(ticker, threshold, start_date, end_date, vectorstore=vectorstore)\n",
        "\n",
        "        if not significant_moves:\n",
        "            return f\"No significant moves (> {threshold}%) found for {ticker} between {start_date} and {end_date}.\"\n",
        "\n",
        "        # Limit the number of significant moves shown to the LLM to prevent overly long context\n",
        "        significant_moves_limited = significant_moves[:5]\n",
        "\n",
        "        # Format significant moves for LLM (re-include concise news snippets for correlation)\n",
        "        moves_context = []\n",
        "        for move in significant_moves_limited:\n",
        "            news_snippets = \"; \".join([s[:50] + '...' if len(s) > 50 else s for s in move['news']]) # Truncate news even further\n",
        "            moves_context.append(\n",
        "                f\"On {move['date']}, {ticker} changed by {move['daily_change_percent'].item():.2f}%. News: {news_snippets}\"\n",
        "            )\n",
        "        moves_context_str = \"\\n\".join(moves_context)\n",
        "\n",
        "        # Refined prompt for significant moves - now using Question/Explanation format\n",
        "        prompt = f\"\"\"\n",
        "Context:\n",
        "Days with price movements greater than {threshold}% for {ticker} from {start_date} to {end_date}:\n",
        "{moves_context_str}\n",
        "\n",
        "Question: Summarize the key significant price movements and their correlation with the provided news articles. Highlight any notable patterns or events observed during this period.\n",
        "Explanation:\n",
        "\"\"\"\n",
        "        return llm.invoke(prompt)\n",
        "\n",
        "    # Existing logic for general queries\n",
        "    date_robust_str = extract_date_robust(user_query) # This returns YYYY-MM or YYYY-MM-DD or YYYY\n",
        "\n",
        "    price_info_str = \"\"\n",
        "    if date_robust_str:\n",
        "        # Convert YYYY-MM or YYYY to a range for get_price_context if necessary\n",
        "        if re.match(r'\\d{4}-\\d{2}$', date_robust_str): # YYYY-MM\n",
        "            year, month = map(int, date_robust_str.split('-'))\n",
        "            start_d = f\"{date_robust_str}-01\"\n",
        "            last_day = calendar.monthrange(year, month)[1]\n",
        "            end_d = f\"{date_robust_str}-{last_day:02d}\"\n",
        "        elif re.match(r'\\d{4}$', date_robust_str): # YYYY\n",
        "            start_d = f\"{date_robust_str}-01-01\"\n",
        "            end_d = f\"{date_robust_str}-12-31\"\n",
        "        else: # Assume YYYY-MM-DD\n",
        "            start_d = date_robust_str\n",
        "            end_d = date_robust_str # For a single day, end_date is the same\n",
        "\n",
        "        price_data_df = get_price_context(ticker, start_date=start_d, end_date=end_d)\n",
        "\n",
        "        if not price_data_df.empty:\n",
        "            # Summarize price data for the date/range\n",
        "            if start_d == end_d: # Single day query\n",
        "                day_data = price_data_df.iloc[0]\n",
        "                price_info_str = f\"{ticker} on {day_data['Date'].item()}: Closed at ${day_data['Close'].item():.2f}, Daily change: {day_data['Daily_Change_Percent'].item():.2f}%.\"\n",
        "            else: # Date range query\n",
        "                first_day = price_data_df.iloc[0]\n",
        "                last_day = price_data_df.iloc[-1]\n",
        "                price_info_str = (\n",
        "                    f\"Historical data for {ticker} from {first_day['Date'].item()} to {last_day['Date'].item()}:\\n\"\n",
        "                    f\"Start Close: ${first_day['Close'].item():.2f}, End Close: ${last_day['Close'].item():.2f}.\\n\"\n",
        "                    f\"Average daily change: {price_data_df['Daily_Change_Percent'].mean():.2f}%.\\n\"\n",
        "                )\n",
        "        else:\n",
        "            price_info_str = f\"Could not find price data for {ticker} around {date_robust_str}.\"\n",
        "\n",
        "        search_query = user_query # Keep original query for news search context\n",
        "\n",
        "    else: # No specific date mentioned in general query\n",
        "        price_data_df = get_price_context(ticker) # Fetch recent 5 days\n",
        "        if not price_data_df.empty:\n",
        "            latest_data = price_data_df.iloc[-1]\n",
        "            price_info_str = f\"{ticker} recently: Closed at ${latest_data['Close'].item():.2f}, Daily change: {latest_data['Daily_Change_Percent'].item():.2f}%.\"\n",
        "        else:\n",
        "            price_info_str = f\"Could not find recent price data for {ticker}.\"\n",
        "        search_query = f\"Recent news about {ticker}\"\n",
        "\n",
        "\n",
        "    news_results = vectorstore.similarity_search(search_query, k=3)\n",
        "    news_context = \"\\n\".join([res.page_content for res in news_results])\n",
        "\n",
        "    # Refined prompt for general queries - now using Question/Explanation format\n",
        "    prompt = f\"\"\"\n",
        "Context:\n",
        "Price Information: {price_info_str}\n",
        "News Articles: {news_context}\n",
        "\n",
        "Question: {user_query}\n",
        "Explanation:\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "# 10. Supervisor Agent Definition\n",
        "class GraphState(BaseModel):\n",
        "    \"\"\"Represents the state of our graph.\"\"\"\n",
        "    query: str = Field(..., description=\"The user's input query\")\n",
        "    agent_outcome: str = Field(None, description=\"The outcome/response from the called agent\")\n",
        "\n",
        "def route_agent(state: GraphState) -> str:\n",
        "    \"\"\"Determines which agent to call based on the user's query.\"\"\"\n",
        "    query = state.query.lower()\n",
        "    if any(keyword in query for keyword in ['chart', 'plot', 'graph', 'visualize']):\n",
        "        return \"call_plotting_agent\"\n",
        "    else:\n",
        "        return \"call_research_agent\"\n",
        "\n",
        "def call_plotting_agent_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"Calls the plotting agent and updates the state with its outcome.\"\"\"\n",
        "    print(f\"Calling Plotting Agent with query: {state.query}\")\n",
        "    outcome = plotting_agent(state.query)\n",
        "    return GraphState(query=state.query, agent_outcome=outcome)\n",
        "\n",
        "def call_research_agent_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"Calls the research agent and updates the state with its outcome.\"\"\"\n",
        "    print(f\"Calling Research Agent with query: {state.query}\")\n",
        "    outcome = research_agent(state.query)\n",
        "    return GraphState(query=state.query, agent_outcome=outcome)\n",
        "\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"call_plotting_agent\", RunnableLambda(call_plotting_agent_node))\n",
        "graph.add_node(\"call_research_agent\", RunnableLambda(call_research_agent_node))\n",
        "graph.set_conditional_entry_point(\n",
        "    route_agent,\n",
        "    {\n",
        "        \"call_plotting_agent\": \"call_plotting_agent\",\n",
        "        \"call_research_agent\": \"call_research_agent\",\n",
        "    },\n",
        ")\n",
        "graph.add_edge(\"call_plotting_agent\", END)\n",
        "graph.add_edge(\"call_research_agent\", END)\n",
        "app = graph.compile()\n",
        "\n",
        "# 11. Modified chatbot_query function\n",
        "def chatbot_query(user_query):\n",
        "    \"\"\"Delegates query processing to the Supervisor agent.\"\"\"\n",
        "    print(f\"Chatbot received query: {user_query}\")\n",
        "    initial_state = GraphState(query=user_query)\n",
        "    result = app.invoke(initial_state)\n",
        "    agent_response = result['agent_outcome']\n",
        "\n",
        "    return agent_response\n",
        "\n",
        "print(\"Consolidated setup and code, including missing helper functions and typo correction.\")\n",
        "\n",
        "# Example Usage (from previous tasks)\n",
        "print(\"\\n--- Example 1: General Query with Month-Year (routed to Research Agent) ---\")\n",
        "print(chatbot_query(\"What happened to stock A in May 2020?\"))\n",
        "\n",
        "print(\"\\n--- Example 2: Significant Moves Query (routed to Research Agent) ---\")\n",
        "print(chatbot_query(\"Show significant moves for A between 2020-01-01 and 2020-06-30 with a 5% threshold.\"))\n",
        "\n",
        "print(\"\\n--- Example 3: Plotting Query (routed to Plotting Agent) ---\")\n",
        "print(chatbot_query(\"Show me a chart of stock A's performance.\"))\n",
        "\n",
        "print(\"\\n--- Example 4: General Query without specific date (routed to Research Agent) ---\")\n",
        "print(chatbot_query(\"Tell me about stock A.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db739f1c"
      },
      "source": [
        "print(chatbot_query(\"Show me a chart of AAPL's performance.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SUMMARY\n",
        "\n",
        "This project developed a sophisticated chatbot capable of providing financial insights and visualizations. Key functionalities include:\n",
        "\n",
        "# Data Loading and Vector Store:\n",
        " The chatbot efficiently loads stock news data from a Hugging Face dataset and creates a FAISS vector store for fast retrieval of relevant news articles.\n",
        "# LLM Integration:\n",
        " It leverages the google/flan-t5-small Language Model to generate human-like summaries and analyses.\n",
        "# Supervisor Agent for Dynamic Routing:\n",
        " A central Supervisor agent intelligently routes user queries to the appropriate specialized sub-agent (Research Agent or Plotting Agent) based on intent.\n",
        "# Research Agent:\n",
        " This agent handles general queries, extracts specific date ranges or provides recent data, and can identify \"significant moves\" in stock prices, correlating them with relevant news articles.\n",
        "# Plotting Agent:\n",
        " For visual requests, this agent generates historical stock price charts using yfinance and matplotlib, returning them as base64 encoded images for direct display.\n",
        "# Robust Date and Ticker Extraction:\n",
        " Helper functions are in place to robustly extract stock tickers and date ranges from user queries.\n",
        "Overall, the chatbot is designed to offer a dynamic and informative experience for users seeking stock market information and analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "2_yrZzt3z78j"
      }
    }
  ]
}